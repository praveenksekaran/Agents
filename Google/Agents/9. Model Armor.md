# Model Armor 

#### Objectives 
- Explain the purpose of Model Armor in a company's security portfolio.
- Define protections that Model Armor applies to all interactions with the LLM.
- Set up the Model Armor API and know where to see violations.
- Identify how Model Armor manages prompts and responses.

## About Model Armor

#### Introduction
Model Armor screens Large Language Model (LLM) prompts and responses for security risks, including prompt injection, data leakage, and malicious content.
Primarily, its job is to screen incoming prompts to the model and outgoing responses from the model, and that's all determined by how you set it up for your needs.
It detects anything that goes against things like responsible AI, prompt injection, jailbreak attempts, or malicious URLs, it will flag them.

[Demo of Model Armor](https://youtu.be/AcZqr7_DA0o)

## LLM security risks

#### OWASP top 10 LLM vulnerabilities
This official Top 10 list of LLM vulnerabilities comes courtesy of the Open Worldwide Application Security Project (OWASP) Foundation.

>Good news: Model Armor takes out four of these major threats! Let's check out what Model Armor is guarding against.

1. **Malicious files and unsafe URLs**
  It's common knowledge that attackers can hijack your LLM by embedding malicious URLs in a prompt. The LLM unwittingly follows the URLs without questioning the intent. Even PDFs can become Trojan horses for unsafe URLs.
The big question is how do you prevent this from happening?
Model Armor's PDF Screening and Malicious URL detection automatically detects and deflects these hidden threats, so you're always covered.

2. **Prompt injection and jailbreaks**
   creative methods to circumvent safety protections and manipulate LLM behavior.
Model Armor is a master at spotting these tricks. Its Prompt Injection and Jailbreak detection is specifically built to catch attempts to bypass security and stop digital troublemakers in their tracks.

[Youtube Demo](https://youtu.be/zviPaxJDlu4)

3. **Sensitive data**
   Remember that special kind of torture when your credit card gets compromised and you're stuck updating all your automatic payments? Nobody wants that headache.
In the same way, customers expect LLMs to keep Personally Identifiable Information (PII) and other sensitive data secure.
That's where Model Armor saves the day! Its Data Loss Prevention (DLP) using Sensitive Data Protection comes with a pre-defined set of basic information types (infoTypes). These infoTypes are designed to find PII, transform it, and keep private details far from prying eyes.

4. **Offensive material**
   In business, reputation is one of your most valuable resources. Allowing someone to hijack your LLM to post offensive material could damage that reputation. Your LLM is nobody's playground.
Enter Model Armor with its Safety and responsible AI filters. These settings allow you to configure strict boundaries, ensuring no unsavory data goes in or out of your LLM.

#### LLM vulnerability & Model Armor feature

| vulnerability | Model Armor feature |
|---|---|
| Hate speech or dangerous content| Responsible AI (RAI) |
| A prompt asking to perform a criminal act | Prompt injection or jail breaking |
| Prompt containing an unsafe URL | malicious URL detection | 
| A response containing a credit card number | Sensitive data protection |

## Customization

#### Making it personal
How does Model Armor offer such tailored protection? Through two core features: floor settings and templates.
Let's start with the golden rule of floor settings: **Always create floor settings before you define templates**.

Go to Model Armor --> Settings and select your level of settings.
Model Armor follows --> Templates which follows --> Floor Settings. 

- **Floor settings**
Floor settings establish the bare minimum security requirements that all your custom configurations within the template must meet. It's the security bedrock.

- **Template**
  A template is your control panel, letting you dial in exactly how Model Armor examines prompts and responses.

## Floor settings 
These settings are the non-negotiable, baseline requirements that every template must follow.
You need consistent protection across all your AI applications. These rules establish the absolute starting point for templates, making it easier for your security pros to maintain a unified front.

#### Configuration 
Want to know your options? You can configure floor settings at three levels in your resource hierarchy.

- **Organization level**
A floor setting at this level adds minimum requirements to all templates associated with any project and any folder inside the organization.
To apply floor settings at this level:
  - Use the resource list at the top of the Google Cloud console.
  - Select an organization from the drop down list of available resources.
  - Create the floor setting.

You can also define the level from the API. For more information, bookmark the help center article titled, "[Enable and disable Model Armor floor settings](https://cloud.google.com/security-command-center/docs/model_armor_floor_settings#enable-disable-ma-floor-settings)".

- **Folder level**
A floor setting at this level adds a minimum requirement to all templates associated with any project inside the folder.
Create the floor setting at this level in the same way. Just select a folder from the Google Cloud console resource list and then create the floor setting.

- **Project level**
You guessed it. A floor setting at this level adds a minimum requirement to all templates associated with a project.
You know what to do. Choose the project from the resource list and start configuring floor settings.

#### Considerations 

Before you decide where to create the floor settings, think about three things:

1. What level is best? Creating at the organization level means that all folders and projects in the organization can inherit those requirements. This makes sense if you want sweeping baseline requirements for everything. Or, you can create them at the folder level or project level and create more specific settings that apply only to projects in that folder or project, respectively.

2. What minimum requirements do I want all templates at that level to follow? Spend a few minutes thinking about those minimum requirements. Templates layer on additional protections, so make your floor settings strategic.

3. Do I want to customize? Do you have a special project or folder that absolutely must have a more relaxed template? With the right permissions, an administrator can break the inheritance to create a custom template or even disable the floor setting requirement altogether.

## Guard rails and confidence levels
Floor settings prevent individual developers from accidentally - or intentionally - lowering security standards below acceptable levels.

1. Imagine this: a security expert lays down the law with a floor setting. Then, a cloud security engineer, perhaps a little too caffeinated, tries to create a template with a confidence level that's less strict than the floor setting minimum.
   What happens next? Model Armor doesn't just politely suggest a change; it throws up an error message and refuses to create or update the template. Model Armor simply says, "Nope! Not gonna happen."
2. Here's some more great news. Templates trying to meander around floor settings aren't just blocked, they are captured and surfaced in Logs Explorer. It's easy to spot and fix templates that aren't following the rules.

#### Teaching Model Armor to trust its gut
Here's another neat trick you can pull with floor settings: you can give Model Armor sensitivity instructions, also known as a confidence level.
Basically, you're telling Model Amor, "Hey, when you spot something shady, you need to be this sure before you flag it as a violation." You're setting its digital "gut feeling" level.

1. Low and above
Model Armor screens almost everything. At this level, it's going to identify issues with the smallest hint of alignment to the detection criteria.

2. Medium and above
Model Armor is a bit more discerning. It flags things that are moderate matches to the detection criteria.

3. High and above
Model Armor is pretty darn confident that the information is a strong match to the detection criteria.

## Templates
Templates tell the API what trouble to flag. When that trouble is identified, it's logged as a violation.
Time to decide what gets to access your LLM and what is screened and left out in the cold.

#### Template Configuration
There are three main parts to a template: **general info, detections and responsible AI**.

You can create templates using the Model Armor API or you can use the Model Armor page in Security Command Center

- general-info
- Detection: amalicious URL
- Detection: Prompt injection and Jail break 
- Detection: Sensitive data
  Chose advance and you create/choose a template for flaging like Credit card etc. and chose a template how to handle it like deidentification 
- Responsible AI

## Model Armor API

#### Set up 
1. Make sure you have right roles 
2. Go to CLI
3. Authorie
4. Client libraries : [Model Armor API client libraries](https://cloud.google.com/security-command-center/docs/reference/model-armor/libraries).
5. Use Python code: [Initialize a Model Armor client in Python](https://cloud.google.com/security-command-center/docs/get-started-model-armor#initialize-model-armor)
6. API Guide. [Model Armor API guide](https://cloud.google.com/security-command-center/docs/reference/model-armor/rest)

## Flagged Violations

#### Logs
Model Armor is a multi-tasker. It's screening the text going in and out of the LLM and it's also taking notes on the activities. These notes are surfaced to you in the form of logs.

There are two types of logs: Admin activity audit logs and Data access logs. Logs are enabled in the template when you're working in the API. To make things easy, bookmark the Help Center article titled, "[Model Armor audit and platform logging](https://cloud.google.com/security-command-center/docs/audit-logging-model-armor)".

1. Admin Activity audit logs capture details about template, floor setting, and basic computing (CRUD) operations.
2. Data access audit logs capture details about screening operations. For example, what template was used to screen a prompt or response, what was the text, and what was the result?

#### Logs Explorer
From the Google Cloud console, choose **Monitoring** and then select **Logs Explorer**. Here are a few filters that are useful for targeting Model Armor logs.

- **protoPayload.serviceName="modelarmor.googleapis.com"**
This filer shows you audit logs that track template actions like create or update.

- **protoPayload.methodName="google.cloud.modelarmor.v1.ModelArmor.SanitizeUserPrompt**
This filter shows you the Data Access audit logs that capture prompt and response screening.

## Quiz
1. Ira asks Ariel, the intern, to help test the Model Armor implementation. Her first step is to enable the API. Where should she go to enable the API? Select all that apply.
   1. She can enable the API from the Google Cloud CLI.
   2. She can enable the API from the Model Armor page of the Security Command Center.

2. Ariel is excited to start working with Model Armor. There are a few things that she needs to do before she can get going. What options can she use to create test templates in Model Armor? Select all that apply.
  1. Python
  2. gcloud CLI
  3. REST with curl commands

3. To support the API testing activities, Ira asks Ariel to configure logging so that he can access the full prompt and response, Model Armorâ€™s evaluation results, and additional metadata fields. How does she do this? Select all that apply.
  1. She can configure logging using a CURL command from a REST API.
  2. She can use Python to send the command to initiate logging.

4. Ira and Ariel are working together to make sure that floor setting requirements are applied across all templates. Ariel decides to test this by creating a new template with a confidence level that is below the requirement enforced by the floor settings. How will Model Armor react to the template violation?
  1. Model Armor shows an error message when Ariel tries to save the new template.

## Prompt & Response
Refer Lab 9: Running Model Armour.ipnyb 

## Application Code

#### decide how to resolve the issues Model Armor discovered

When using the Model Armor REST API, you can write application code to perform a few simple steps. 

1. Make a call. Call Model Armor to screen user prompts and model responses.
2. Read responses. Parse Model Armor's responses after each completed call.
3. Decide what to do. Think about the best way to respond to what Model Armor provided. You have a few choices:
  - Block the user's request.
  - Block the model's response.
  - Warn the user.
  - Use redacted text and continue your workflow.

      ```
      # pip install google-cloud-modelarmor
      from google.cloud import modelarmor_v1
      import sys
      
      # Create a client
      client = modelarmor_v1.ModelArmorClient(transport="rest", client_options = {"api_endpoint" : "modelarmor.us-central1.rep.googleapis.com"})
      
      # Initialize request argument(s)
      user_prompt_data = modelarmor_v1.DataItem()
      
      # Get the prompt from command line argument
      if len(sys.argv) > 1: # Check if an argument is provided
          prompt = sys.argv[1] # Take the first argument as the prompt
      else:
          # Fallback to a default prompt if no argument is provided
          prompt = "Placeholder prompt."
      
      # Set prompt data for model armor call
      user_prompt_data.text = prompt
      ma_request = modelarmor_v1.SanitizeUserPromptRequest(
          name="projects/csa-model-armor-demo-012346/locations/us-central1/templates/pijb-only", # name contains the project and template
          user_prompt_data=user_prompt_data,
      )
      
      # Make the MA request
      ma_response = client.sanitize_user_prompt(request=ma_request)
      
      # Take action based on Model Armor's result
      if ma_response.sanitization_result.filter_results["pi_and_jailbreak"].pi_and_jailbreak_filter_result.match_state == modelarmor_v1.FilterMatchState.MATCH_FOUND: # A PIJB match was found
          print("Query failed security check. Error.")
      else:
          print("Query passed security check. Sending prompt to LLM.")
      ```

